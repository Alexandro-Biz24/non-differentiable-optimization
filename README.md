# Non-differentiable-optimization :

This notebook explores the behavior, tuning, and intuition behind **gradient descent** optimization techniques. It is a pedagogical and experimental project to better understand how gradient-based methods behave under different conditions and parameters.

---

## Files

- `Research_sous_gradient_.ipynb`  
  Main notebook containing the core experiments and visualizations on gradient descent.

---

## Objectives

- Implement basic and custom gradient descent algorithms
- Visualize the convergence behavior
- Test learning rate effects
- Observe the impact of curvature (loss surface shape)
- Develop a better intuition of optimization dynamics

---

## Technologies Used

- Python 3.10+
- `numpy`, `matplotlib`, `pandas`
- Jupyter Notebook for interactive visualization

---
Click on 'Research_sous_gradient_.ipynb' to see the project

Alexandro Bizeul
